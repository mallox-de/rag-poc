# RAG PoC ‚Äì Lokale Wissensdatenbank mit Ollama & Qdrant

Dieses Projekt ist ein **Proof of Concept (PoC)** f√ºr eine **lokal laufende, abfragbare Wissensdatenbank (RAG ‚Äì Retrieval Augmented Generation)**.

Ziel ist es:
- strukturierte und unstrukturierte Dokumente (PDF, HTML, Markdown, TXT, DOCX, RTF) zu indexieren
- semantische und lexikalische Suche zu kombinieren
- Antworten **nachvollziehbar mit Quellen & Textausschnitten** zu erzeugen
- alles **lokal**, reproduzierbar und ohne Cloud-Abh√§ngigkeit zu betreiben

---

## Architektur-√úberblick

**Kernidee:**
1. Dokumente werden lokal eingelesen und normalisiert
2. Texte werden in Chunks zerlegt
3. Jeder Chunk wird:
   - als **Vektor** (Dense Search) in Qdrant gespeichert
   - als **Text** f√ºr BM25 (lexikalische Suche) persistiert
4. Abfragen kombinieren beide Sucharten (Hybrid Retrieval)
5. Ein LLM erzeugt daraus eine konsolidierte Antwort

---

## Komponenten & Aufgaben

### üß† Ollama
Lokaler Model-Server f√ºr:
- **Embeddings** (`bge-m3`, multilingual)
- **Chat-Modelle** (z. B. `mistral:7b-instruct`, `qwen2.5:7b-instruct`)

Wird genutzt f√ºr:
- Erzeugung von Vektoren (Embedding)
- Zusammenfassung und Antwortgenerierung

---

### üóÇ Qdrant
Vektor-Datenbank f√ºr:
- Speicherung der Embeddings
- Metadaten (Quelle, Chunk-Index, Text, Dateityp)
- schnelle semantische √Ñhnlichkeitssuche

Zus√§tzlich:
- Dashboard zur Inspektion der Inhalte

---

### üìö BM25 (lokal)
Lexikalischer Suchindex (Classic IR):
- speichert tokenisierte Chunk-Texte
- erlaubt exakte Wort- und Stamm-Suche
- dient als **Gegenpr√ºfung** f√ºr semantische Treffer

Persistiert in:
```
.bm25_chunks.jsonl
```

---

### üìÑ Dokumentenverarbeitung
Unterst√ºtzte Formate:
- **TXT / MD / Markdown** ‚Äì direkt
- **HTML** ‚Äì via BeautifulSoup (Tags, Scripts entfernt)
- **PDF** ‚Äì via pypdf inkl.:
  - Inhaltsverzeichnis-Erkennung
  - Header/Footer-Erkennung
- **DOCX** ‚Äì via python-docx (rein Python)
- **RTF** ‚Äì via striprtf (rein Python)

Nicht robust unterst√ºtzt:
- **DOC (Word 97‚Äì2003)** ‚Üí Empfehlung: vorher in DOCX oder PDF konvertieren

---

### üß© Chunking & Normalisierung
- Zeichenbasiertes Chunking (PoC-freundlich)
- konfigurierbare Gr√∂√üe & Overlap
- Deduplizierung auf Chunk-Ebene

Standard:
- Chunk-Gr√∂√üe: 900 Zeichen
- Overlap: 120 Zeichen

---

### üîç Retrieval-Strategie

**Hybrid Retrieval:**
1. Dense Search (Qdrant + Embeddings)
2. BM25 Search (lexikalisch)
3. **RRF (Reciprocal Rank Fusion)** zur Kombination

Vorteile:
- semantische √Ñhnlichkeit + exakte Begriffe
- robuste Ergebnisse auch bei Fachterminologie

---

## Funktionsweise der Suche & Kombination

Zus√§tzlich wird eine **deterministische Synonym-Behandlung (Query Expansion)** eingesetzt, um Fachbegriffe, Abk√ºrzungen und unterschiedliche Bezeichnungen robust abzudecken, ohne den Index aufzubl√§hen.

Dieser Abschnitt erkl√§rt die drei zentralen Bausteine des Retrievals und wie sie zusammenwirken.

---

### Dense Search (Qdrant + Embeddings)

**Was passiert?**  
Bei der Dense Search wird sowohl der **Dokument-Chunk** als auch die **Nutzerfrage** in einen hochdimensionalen Vektorraum eingebettet.

- Embeddings werden √ºber **Ollama** mit einem multilingualen Modell (z. B. `bge-m3`) erzeugt
- Jeder Chunk wird als Vektor in **Qdrant** gespeichert
- Die Anfrage wird ebenfalls eingebettet
- Qdrant berechnet die **kosinusbasierte √Ñhnlichkeit** zwischen Query-Vektor und Chunk-Vektoren

**Eigenschaften**
- erkennt **semantische √Ñhnlichkeit**
- robust gegen√ºber Synonymen und Paraphrasen
- funktioniert sprach√ºbergreifend (DE/EN)

**Grenzen**
- ungenau bei exakten Begriffen (IDs, Klassennamen)
- kann semantisch ‚Äû√§hnliche‚Äú, aber faktisch falsche Treffer liefern

---

### BM25 Search (lexikalisch)

**Was passiert?**  
BM25 ist ein klassischer Information-Retrieval-Algorithmus, der auf **Token-H√§ufigkeiten** basiert.

- Texte werden tokenisiert (optional mit Stemming DE/EN)
- H√§ufige W√∂rter werden geringer gewichtet (IDF)
- Treffer werden nach Relevanz-Score sortiert

**Eigenschaften**
- sehr gut f√ºr:
  - exakte Begriffe
  - Fachterminologie
  - Abk√ºrzungen
- vollst√§ndig deterministisch und erkl√§rbar

**Grenzen**
- erkennt keine Synonyme
- anf√§llig f√ºr unterschiedliche Wortformen ohne Stemming
- keine semantische Generalisierung

---

### RRF ‚Äì Reciprocal Rank Fusion

**Was passiert?**  
RRF kombiniert mehrere unabh√§ngige Rankings zu einer gemeinsamen Ergebnisliste.

Formel (vereinfacht):
```
Score = Œ£ 1 / (k + Rang)
```

Dabei:
- jeder Treffer erh√§lt Punkte basierend auf seiner Position im jeweiligen Ranking
- `k` ist eine Konstante zur Gl√§ttung (z. B. 60)

**Warum RRF?**
- robust gegen Ausrei√üer
- keine Score-Normalisierung n√∂tig
- funktioniert gut bei unterschiedlich skalierten Scores (Dense vs. BM25)

**Effekt im PoC**
- Treffer, die in **beiden** Suchen gut ranken, steigen nach oben
- Einseitig starke Treffer bleiben sichtbar, dominieren aber nicht

---

### Synonyme & Query Expansion

**Problemstellung**  
In technischen Dokumentationen treten h√§ufig Synonyme, Abk√ºrzungen und unterschiedliche Bezeichnungen auf, z.‚ÄØB.:
- ‚ÄûSSO‚Äú ‚Üî ‚ÄûSingle Sign-On‚Äú ‚Üî ‚ÄûEinmalanmeldung‚Äú
- ‚ÄûRBAC‚Äú ‚Üî ‚Äûrollenbasierte Zugriffskontrolle‚Äú

Weder reine Dense Search noch BM25 l√∂sen dieses Problem vollst√§ndig zuverl√§ssig.

---

**Gew√§hlte L√∂sung**: *Query Expansion zur Laufzeit*

Statt Dokumente oder Chunks zu duplizieren, wird **die Nutzerfrage erweitert**, bevor die Suche ausgef√ºhrt wird.

- Synonyme werden aus einer **kuratieren JSON-Datei** geladen (`synonyms.json`)
- Die Erweiterung erfolgt:
  - **deterministisch** (kein LLM)
  - **transparent** (optional per `--verbose` sichtbar)
  - **nur bei Bedarf**

Beispiel:
```text
Frage:  Wie funktioniert SSO?
BM25:   Wie funktioniert SSO single sign-on einmalanmeldung
Dense:  Wie funktioniert SSO   (optional ebenfalls expandiert)
```

---

**Warum keine Chunk-Duplizierung?**
- vermeidet k√ºnstliches Aufbl√§hen des Index
- verhindert doppelte Treffer
- erh√§lt Zitierf√§higkeit (Originaltext bleibt unver√§ndert)

---

**Konfiguration**
- `--synonyms-file synonyms.json` ‚Üí aktiviert Query Expansion
- `--expand-dense` ‚Üí optional auch Dense Search erweitern

Diese Strategie erg√§nzt Hybrid Search ideal und erh√∂ht die Trefferqualit√§t insbesondere bei Fachterminologie.

---

### Zusammenspiel im System

1. Nutzer stellt eine Frage
2. Dense Search liefert semantische Treffer
3. BM25 liefert lexikalische Treffer
4. RRF fusioniert beide Rankings
5. Top-N Chunks werden als Kontext an das LLM gegeben

Das Ergebnis ist ein **robustes, erkl√§rbares und gut kontrollierbares Retrieval**.

---

### üß† Antwortgenerierung
- LLM bekommt **nur relevante Chunks**
- Antwort wird konsolidiert und neutral formuliert
- Referenzen werden **deterministisch** aus Python ausgegeben
- Chunk-Texte k√∂nnen optional angezeigt werden

---

## State & Persistenz

### `.rag_state.json`
- merkt sich `file_hash` pro Datei
- erlaubt **inkrementelles Reindexing**

### `.bm25_chunks.jsonl`
- persistierter BM25-Korpus
- wird bei Bedarf neu aufgebaut

---

## Setup

### Python-Abh√§ngigkeiten
```bash
pip install qdrant-client requests beautifulsoup4 lxml \
            rank-bm25 snowballstemmer pypdf \
            python-docx striprtf typer
```

---

### Ollama installieren
```bash
brew install ollama
```

Modelle laden:
```bash
ollama pull bge-m3
ollama pull mistral:7b-instruct
# optional
ollama pull qwen2.5:7b-instruct
```

---

### Qdrant starten (Docker)
```bash
mkdir -p qdrant_storage

docker run -d \
  -p 6333:6333 -p 6334:6334 \
  -v "$(pwd)/qdrant_storage:/qdrant/storage:z" \
  qdrant/qdrant
```

Dashboard:
```
http://localhost:6333/dashboard
```

---

## Nutzung

### Index aufbauen
```bash
python rag.py ingest
```

### Abfrage stellen
```bash
python rag.py query "Welche Architekturprinzipien gibt es?"
```

### Mit Referenzen & Chunk-Text
```bash
python rag.py query "‚Ä¶" --refs
```

### State zur√ºcksetzen
```bash
python rag.py reset-state --delete-bm25
```

---

## Performance-Hinweise

Empfohlene Startwerte:
- `top_out`: 4‚Äì5
- `max_chars`: 900‚Äì1100
- kleinere Chat-Modelle f√ºr PoC

F√ºr Apple Silicon:
- Ollama **nativ** (nicht im Docker) ist deutlich schneller

---

## Komponenten & Workflow

### Komponenten
```mermaid
graph LR
  U[User] --> A[rag.py]

  subgraph Local
    A -->|embed| O[Ollama<br/>Embeddings]
    A -->|chat| L[Ollama<br/>LLM]
    A -->|dense search| Q[(Qdrant)]
    A -->|bm25| B[(BM25 Index)]
    A --> FS[Filesystem<br/>data/**]
    A --> ST[State<br/>.rag_state.json]
  end
```

### Workflow
```mermaid
flowchart TD
  SYN[Synonym-Expansion
(Query)] --> QUERY
  ING[Ingest] --> SCAN[Scan Dateien]
  SCAN --> PARSE[Format-spezifische Extraktion]
  PARSE --> CHUNK[Chunking]
  CHUNK --> EMB[Embeddings]
  EMB --> QDR[Qdrant Upsert]
  CHUNK --> BM25[BM25 Persistenz]

  QUERY[Query] --> DENSE[Dense Search]
  QUERY --> LEX[BM25 Search]
  DENSE --> FUSE[RRF Fusion]
  LEX --> FUSE
  FUSE --> CTX[Kontext]
  CTX --> LLM[LLM Antwort]
```

---

## Architekturentscheidungen (ADR)

Dieses Kapitel dokumentiert bewusst getroffene Architekturentscheidungen f√ºr den PoC und deren Begr√ºndung.

### ADR-001: Hybrid Retrieval (Dense + BM25)

**Entscheidung**  
Es wird eine **Hybrid-Suche** aus semantischer Vektorsuche (Dense Retrieval) und klassischer lexikalischer Suche (BM25) eingesetzt. Die Ergebnisse werden mittels **Reciprocal Rank Fusion (RRF)** kombiniert.

**Begr√ºndung**
- Semantische Suche allein ist anf√§llig f√ºr:
  - Fachbegriffe
  - Abk√ºrzungen
  - exakte Formulierungen (z. B. Klassennamen, Architekturpattern)
- BM25 allein ist anf√§llig f√ºr:
  - Synonyme
  - Paraphrasen
  - unterschiedliche Sprachen (DE/EN)

Durch die Kombination:
- werden **False Negatives** reduziert
- steigt die Robustheit bei heterogenen Dokumenten
- bleibt das System erkl√§rbar und deterministisch

**Alternativen**
- Nur Dense Search ‚Üí schlechter bei exakten Begriffen
- Nur BM25 ‚Üí schlechter bei semantischen Fragen
- Learned Ranker ‚Üí zu komplex f√ºr PoC

**Status**: akzeptiert

---

### ADR-002: Kein Fine-Tuning des LLM

**Entscheidung**  
Es wird **kein Fine-Tuning** eines Sprachmodells durchgef√ºhrt.

**Begr√ºndung**
- Ziel des PoC ist **Wissenszugriff**, nicht Wissenslernen
- Fine-Tuning:
  - ist daten- und zeitintensiv
  - erschwert Reproduzierbarkeit
  - verschlechtert oft Faktenpr√§zision
- RAG trennt sauber:
  - **Wissen** (Dokumente)
  - **F√§higkeiten** (LLM)

Das Modell wird ausschlie√ülich genutzt f√ºr:
- Sprachverst√§ndnis
- Zusammenfassung
- Konsolidierung mehrerer Textstellen

**Alternativen**
- LoRA / PEFT ‚Üí sinnvoll erst bei stabilen Use-Cases
- Instruction-Tuning ‚Üí evtl. sp√§ter f√ºr Ton/Format

**Status**: akzeptiert

---

### ADR-003: Chunking auf Zeichenbasis

**Entscheidung**  
Chunks werden zeichenbasiert (statt tokenbasiert) erzeugt.

**Begr√ºndung**
- keine Abh√§ngigkeit von Modell-Tokenizern
- stabil √ºber Modellwechsel hinweg
- ausreichend pr√§zise f√ºr PoC-Zwecke

**Trade-off**
- weniger exakt bzgl. Kontextfenster
- wird bewusst in Kauf genommen

**Status**: akzeptiert

---

### ADR-004: Deterministische Referenzen au√üerhalb des LLM

**Entscheidung**  
Referenzen und Chunk-Texte werden **nicht vom LLM generiert**, sondern im Python-Code ausgegeben.

**Begr√ºndung**
- vermeidet Halluzinationen
- garantiert Nachvollziehbarkeit
- erm√∂glicht Audits und Debugging

**Status**: akzeptiert

---

## Ziel des PoC

- Nachvollziehbare, zitierf√§hige Antworten
- Lokale Ausf√ºhrung
- Klare Trennung von Retrieval & Generierung
- Erweiterbar (UI, API, weitere Formate, Metadaten)

---

Wenn du m√∂chtest, kann dieses README im n√§chsten Schritt noch erg√§nzt werden um:
- Architektur-Entscheidungen (ADR-Stil)
- Grenzen & bekannte Trade-offs
- Erweiterungsideen (UI, FastAPI, Auth, Multi-Collection)

